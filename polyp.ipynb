{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "\n",
    "def setup(required):\n",
    "    required = set(required)\n",
    "    installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "    missing   = required - installed\n",
    "    if missing:\n",
    "        # implement pip as a subprocess:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', *missing])\n",
    "        \n",
    "setup({'numpy', 'matplotlib', 'pandas', 'torch', 'wandb', 'tqdm', 'opencv-python', 'torchvision', })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "import torch\n",
    "\n",
    "INPUT_DIR = r\"C:\\Users\\nmttu\\Downloads\\bkai-igh-neopolyp\"\n",
    "OUTPUT_DIR = \"out\"\n",
    "TRAIN_DIR = f\"{INPUT_DIR}/train/train\"\n",
    "TEST_DIR = f\"{INPUT_DIR}/test/test\"\n",
    "CHECKPOINT_DIR = r\"checkpoint\"\n",
    "CHECKPOINT_PATH = r\"scratch_11152023_121350_5.pth\"\n",
    "\n",
    "num_workers = 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "learning_rate = 10**-3\n",
    "end_learning_rate = 10*678*-5\n",
    "num_epochs = 50\n",
    "batch_size = 1\n",
    "img_size = (736, 960)\n",
    "pretrain = \"scratch\"\n",
    "split_ratio = [0.8, 0.2]\n",
    "loss_func = torch.nn.CrossEntropyLoss(weight=torch.tensor([0.45, 0.45, 0.1], device=device))\n",
    "\n",
    "report_step = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "import math\n",
    "import wandb\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "def argmax2img(arr):\n",
    "    if len(arr.shape) == 4:\n",
    "        return torch.stack([argmax2img(img) for img in arr])\n",
    "    img = torch.zeros(3, arr.shape[-2], arr.shape[-1])\n",
    "    red = arr == 0\n",
    "    green = arr == 1\n",
    "    img[0, :, :] = red\n",
    "    img[1, :, :] = green\n",
    "    return img.float()\n",
    "\n",
    "def transform(img, gt, random_seed: int = None):\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        torch.manual_seed(random_seed)\n",
    "    transformation = transforms.Compose([transforms.RandomPerspective(p = 1),\n",
    "                                          transforms.RandomRotation(degrees = math.pi / 4,expand = True),])\n",
    "    stacked = torch.stack(tensors = [img, gt], dim = 0)\n",
    "    stacked = transformation(stacked)\n",
    "    img, gt = stacked[0, :, :], stacked[1, :, :]\n",
    "    img = transforms.ColorJitter()(img)\n",
    "    return img, gt\n",
    "\n",
    "def train(dataloader, model, num_epochs = 100, learning_rate = 10**-3, loss_func = None, pretrain_name=\"scratch\", start=0, end_learning_rate = None, report_step = 1000, vali_dataloader = None):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    if end_learning_rate is not None:\n",
    "        expo = (end_learning_rate / learning_rate) ** (1/num_epochs)\n",
    "    else:\n",
    "        expo = 1\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = expo)\n",
    "    \n",
    "    accu_loss = 0\n",
    "    for e in range(1 + start, 1 + start+num_epochs):\n",
    "        for idx, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            y = TF.center_crop(y, [pred.shape[-2], pred.shape[-1]])\n",
    "            loss = loss_func(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch {e}/{num_epochs} --- Batch {idx + 1}/{len(dataloader)} --- Loss {loss.item():.4f}\", end='\\r')\n",
    "            accu_loss+= loss.detach()\n",
    "            if ((idx + 1)*dataloader.batch_size) % report_step == 0:\n",
    "                wandb.log({\"Train Loss\": accu_loss.item() / report_step})\n",
    "                accu_loss = 0\n",
    "                table = wandb.Table(columns=[\"Predict\", \"Target\"])\n",
    "                table.add_data(wandb.Image(pred[0]),\n",
    "                               wandb.Image(argmax2img(y[0])))\n",
    "                wandb.log({f\"Comparision\": table})\n",
    "\n",
    "                # Validation\n",
    "                if vali_dataloader is not None:\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        vali_loss = 0\n",
    "                        for idx, (x, y) in enumerate(vali_dataloader):\n",
    "                            x, y = x.to(device), y.to(device)\n",
    "                            pred = model(x)\n",
    "                            y = TF.center_crop(y, [pred.shape[-2], pred.shape[-1]])\n",
    "                            vali_loss += loss_func(pred, y)\n",
    "                        vali_loss /= len(vali_dataloader)\n",
    "                        wandb.log({\"Validation Loss\": vali_loss.item()})\n",
    "                    model.train()\n",
    "        scheduler.step()\n",
    "        \n",
    "        checkpoint_path = f\"{pretrain_name}_{datetime.now(tz=timezone(timedelta(hours=7))).strftime(r'%m%d%Y_%H%M%S')}_{e}.pth\"\n",
    "        torch.save({\"model\":model,\n",
    "                        \"optimizer_state_dict\":optimizer.state_dict()}, os.path.join(CHECKPOINT_DIR, checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, directory:str, transform=None, standard_size = (608, 800), train = True):\n",
    "        '''Param:\n",
    "        `dir` (string): Path to the directory containing samples. This directory cannot contain any other files.\n",
    "        `name` (string, optional): Name of the dataset\n",
    "        `transform` (callable, optional): The transformation to be applied to a sample'''\n",
    "\n",
    "        self.__train = train\n",
    "        self.transform = transform\n",
    "        self.__standard_size = standard_size # Sketch images to one standard size. 1280x1024 by default.\n",
    "        self.__list_samples__ = [] # A list containing samples' filenames.\n",
    "        self.__list_gt__ = [] # A list containing grouth truth images' filenames.\n",
    "        \n",
    "        try:\n",
    "            if self.__train:\n",
    "                list_files = os.listdir(f\"{directory}/train/train\")\n",
    "                for file_name in list_files: # Sample and ground truth pair should have the same name.\n",
    "                    self.__list_samples__.append(f'{directory}/train/train/{file_name}')\n",
    "                    self.__list_gt__.append(f'{directory}/train_gt/train_gt/{file_name}')\n",
    "            else:\n",
    "                list_files = os.listdir(f\"{directory}/test/test\")\n",
    "                for file_name in list_files:\n",
    "                    self.__list_samples__.append(f'{directory}/test/test/{file_name}')\n",
    "        except FileNotFoundError:\n",
    "            err_msg = f\"Directory {directory} does not exist!\"\n",
    "            raise FileNotFoundError(err_msg)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.__list_samples__)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = read_image(self.__list_samples__[idx]) / 255\n",
    "        if self.__train:\n",
    "            gt = read_image(self.__list_gt__[idx]) / 255\n",
    "        else:\n",
    "            gt = torch.zeros(size=sample.shape)\n",
    "        if self.transform:\n",
    "            sample, gt = self.transform(sample, gt)\n",
    "        sample = TF.resize(sample, self.__standard_size)\n",
    "        gt = TF.resize(gt, self.__standard_size)\n",
    "        gt = ((gt - 0.8) > 0).float()\n",
    "        # To class label\n",
    "        red = gt[0]\n",
    "        green = gt[1]\n",
    "        background = 1 - (red + green) # No red or no green --> background = 1\n",
    "        gt = green + background*2 # gt[..] = 0 means red, gt[..] = 1 means green, gt[..] = 2 means background\n",
    "        return sample, gt.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class ConvReLU(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 3, pre_activation = None, padding = 0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = kernel_size, padding = padding)\n",
    "        self.batchnorm = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        if pre_activation is None or not isinstance(pre_activation, nn.Module):\n",
    "            self.pre_activation = nn.Identity()\n",
    "        else:\n",
    "            self.pre_activation = pre_activation\n",
    "    def forward(self, x):\n",
    "        x = self.pre_activation(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class StackedConvReLU(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stack = 2, kernel_size = 3, pre_activation = None, padding = 0):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential(ConvReLU(in_channels, out_channels, kernel_size, padding=padding))\n",
    "        for _ in range(stack - 1):\n",
    "            self.stack.append(ConvReLU(out_channels, out_channels, kernel_size, padding=padding))\n",
    "        if pre_activation is None or not isinstance(pre_activation, nn.Module):\n",
    "            self.pre_activation = nn.Identity()\n",
    "        else:\n",
    "            self.pre_activation = pre_activation\n",
    "    def forward(self, x):\n",
    "        x = self.pre_activation(x)\n",
    "        return self.stack(x)\n",
    "    \n",
    "class Up(StackedConvReLU):\n",
    "    def __init__(self, in_channels, out_channels, stack = 2, kernel_size = 3, pre_activation = None, padding = 1):\n",
    "        '''A child class with modifications so that the forward will concat 2 tensors before send them to self.stack.foward'''\n",
    "        super().__init__(in_channels=in_channels, out_channels=out_channels, stack=stack, kernel_size=kernel_size, pre_activation=pre_activation, padding=padding)\n",
    "    def forward(self, x, skip):\n",
    "        x = self.pre_activation(x)\n",
    "        skip = TF.center_crop(skip, [x.shape[-2], x.shape[-1]])\n",
    "        stacked = torch.cat([x, skip], dim=1)\n",
    "        return self.stack(stacked)\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, encoder_channels: list, in_channels: int = 3, out_channels: int = 3):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(StackedConvReLU(in_channels, encoder_channels[0]))\n",
    "        self.encoder.extend([StackedConvReLU(encoder_channels[idx],\n",
    "                                             encoder_channels[idx + 1],\n",
    "                                             pre_activation = nn.MaxPool2d(2)) for idx in range(0, len(encoder_channels) - 1)])\n",
    "        \n",
    "        self.decoder = nn.Sequential(*[Up(encoder_channels[idx],\n",
    "                                                       encoder_channels[idx - 1],\n",
    "                                                       pre_activation = nn.ConvTranspose2d(encoder_channels[idx], encoder_channels[idx - 1] , 2 , 2)) for idx in range(len(encoder_channels)-1, 0, -1)]) \n",
    "        self.head = nn.Conv2d(in_channels = encoder_channels[0], out_channels = out_channels, kernel_size = 1)\n",
    "    def forward(self, x):\n",
    "        encode = []\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            encode.insert(0, x)\n",
    "        encode = encode[1:]\n",
    "\n",
    "        for idx, layer in enumerate(self.decoder):\n",
    "            x = layer(x, encode[idx])\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "train_img = ImageDataset(directory=INPUT_DIR, train=True, transform=transform, standard_size=img_size)\n",
    "train_set, vali_set = torch.utils.data.random_split(train_img, split_ratio)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers,)\n",
    "vali_dataloader = torch.utils.data.DataLoader(vali_set, batch_size=batch_size, shuffle=True, num_workers=num_workers,)\n",
    "model = UNet([16, 32, 64, 128, 256], 3, )\n",
    "model.to(device)\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=\"IntroToDLSegmentation\",\n",
    "    name=f'{pretrain}_weightedCE_{datetime.now(tz=timezone(timedelta(hours=7)))}',\n",
    "    config={\"learning_rate\":learning_rate,\n",
    "            \"architecture\": \"UNet\",\n",
    "            \"dataset\": \"BK NeoPolyp\",\n",
    "            \"epochs\": num_epochs,\n",
    "           },\n",
    ")\n",
    "train(train_dataloader, model, num_epochs = num_epochs, learning_rate = learning_rate, loss_func= loss_func, pretrain_name=pretrain, end_learning_rate = end_learning_rate, report_step = report_step, vali_dataloader=vali_dataloader)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.io import read_image, write_png\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "try:\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "#model = torch.load(f\"{CHECKPOINT_DIR}/{CHECKPOINT_PATH}\", map_location=device)[\"model\"]\n",
    "# Use trained model above\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for file_name in tqdm(os.listdir(TEST_DIR)):\n",
    "        img = read_image(f\"{TEST_DIR}/{file_name}\") / 255\n",
    "        original_size = [val for val in img.shape[:-1]] \n",
    "        resized = TF.resize(img.unsqueeze(0), img_size) \n",
    "        pred = model(resized.to(device)).detach().cpu() \n",
    "        \n",
    "        resized_pred = TF.resize(pred, original_size)\n",
    "        res = (argmax2img(resized_pred[0].argmax(dim=0)) * 255).type(torch.uint8)\n",
    "        write_png(res, f\"{OUTPUT_DIR}/{file_name[:-5]}.png\", compression_level=0)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def rle_to_string(runs):\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_encode_one_mask(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels[pixels > 0] = 255\n",
    "    use_padding = False\n",
    "    if pixels[0] or pixels[-1]:\n",
    "        use_padding = True\n",
    "        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n",
    "        pixel_padded[1:-1] = pixels\n",
    "        pixels = pixel_padded\n",
    "    \n",
    "    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    if use_padding:\n",
    "        rle = rle - 1\n",
    "    rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "    return rle_to_string(rle)\n",
    "\n",
    "def mask2string(dir):\n",
    "    \n",
    "    strings = []\n",
    "    ids = []\n",
    "    ws, hs = [[] for i in range(2)]\n",
    "    for image_id in os.listdir(dir):\n",
    "        id = image_id.split('.')[0]\n",
    "        path = os.path.join(dir, image_id)\n",
    "        print(path)\n",
    "        img = cv2.imread(path)[:,:,::-1]\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        for channel in range(2):\n",
    "            ws.append(w)\n",
    "            hs.append(h)\n",
    "            ids.append(f'{id}_{channel}')\n",
    "            string = rle_encode_one_mask(img[:,:,channel])\n",
    "            strings.append(string)\n",
    "    r = {\n",
    "        'ids': ids,\n",
    "        'strings': strings,\n",
    "    }\n",
    "    return r\n",
    "\n",
    "\n",
    "dir = OUTPUT_DIR\n",
    "res = mask2string(dir)\n",
    "df = pd.DataFrame(columns=['Id', 'Expected'])\n",
    "df['Id'] = res['ids']\n",
    "df['Expected'] = res['strings']\n",
    "df.to_csv(r'output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
